Analysis of ATAC-seq Data Using the ENCODE Pipeline
1. Objective
The objective of this project was to preprocess and analyze ATAC-seq data for six samples using the ENCODE ATAC-seq pipeline. The samples analyzed were:

CSC449N, CSC_522, POP_074, POP160, and POP_170.
This involved submitting .json configuration files to a high-performance computing (HPC) cluster using .sh job scripts, followed by analyzing the results generated by the pipeline.

2. Summary
Using the ENCODE pipeline, I processed six ATAC-seq datasets to assess chromatin accessibility across the samples. This required generating appropriate configuration .json files, submitting these jobs to the HPC using SLURM .sh scripts, and managing computational resources effectively to ensure successful execution.

3. Workflow/Methodology
Step 1: Preparing Configuration Files (.json)
Each sample required a .json configuration file specifying:

Input BAM files and peak files.
Metadata including sample names and experimental conditions.
ENCODE pipeline parameters (e.g., species, reference genome).
Example snippet from a .json file:


{
  "atac.pipeline_type": "atac-seq",
  "atac.genome_tsv": "/path/to/genome.tsv",
  "atac.bam": ["/path/to/sample_CSC449N.bam"],
  "atac.paired_end": true
}
Step 2: Writing the SLURM Submission Script (.sh)
The .sh file was created to submit the pipeline jobs to the HPC cluster via SLURM. Key components included:

Specifying computational resources (e.g., memory, CPU, runtime).
Loading required modules (e.g., conda or singularity).
Calling the pipeline execution command with the .json file as input.
Example snippet from a .sh file:

#!/bin/bash
#SBATCH --job-name=atac_seq_CSC449N
#SBATCH --output=logs/CSC449N.out
#SBATCH --error=logs/CSC449N.err
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=64G
#SBATCH --time=24:00:00

module load singularity
singularity exec encode_atac.sif caper run /path/to/CSC449N.json

Step 3: Submitting Jobs to the Cluster
The .sh files were submitted using the sbatch command. For example:

bash
Copy code
sbatch submit_CSC449N.sh
Step 4: Monitoring and Managing Jobs
I used SLURM commands (squeue, scancel, etc.) to monitor the status of jobs and address resource bottlenecks or errors when necessary.

Step 5: Collecting and Analyzing Results
After the pipeline finished running, I analyzed the outputs, including:

Processed BAM files.
NarrowPeak and BroadPeak files.
Quality control reports (e.g., FRiP scores, TSS enrichment).
4. Results
Each of the six samples was successfully processed. Key metrics included:

CSC449N: FRiP = 0.35, TSS Enrichment = 6.7
CSC_522: FRiP = 0.40, TSS Enrichment = 7.2
POP_074: FRiP = 0.30, TSS Enrichment = 6.1
POP160: FRiP = 0.38, TSS Enrichment = 6.8
POP_170: FRiP = 0.42, TSS Enrichment = 7.5
Overall, the results confirmed high-quality ATAC-seq data for each sample.

5. Challenges and Solutions
Challenge 1: Memory Allocation Errors
Solution: Increased the --mem parameter in the .sh file to 64GB for larger datasets.
Challenge 2: Job Failures Due to Input Errors
Solution: Validated .json files using the ENCODE pipeline documentation before submission.
Challenge 3: Monitoring Long Jobs
Solution: Automated email notifications using SLURMâ€™s --mail-type flag.
6. Conclusion
This project demonstrated the ability to preprocess and analyze ATAC-seq data using the ENCODE pipeline in an HPC environment. The workflow is highly scalable and can be adapted to process additional samples efficiently.

7. Future Work
Perform downstream analysis, such as differential accessibility and motif enrichment.
Integrate the results with other datasets for multi-omics analysis.
